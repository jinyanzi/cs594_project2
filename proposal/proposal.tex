\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{CS 594 Project 2 Proposal: Depth and Width Study of Neural Network for Image Recognition}


\author{
Yanzi Jin \\
\texttt{yjin25z@uic.edu.edu} \\
\And
Yaru Shi \\
\texttt{yshi31@uic.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

We are going to reproduce part of the experiment of Deep Residual Network (resNet) \cite{he2015deep} and wide Residual Network (WRN) \cite{zagoruyko2016wide} for image recognition. Compared to other framework like GoogLeNet \cite{szegedy2015going}, resNet has much simpler architecture, and achieves the state-of-the-art performance. It won the first place of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation by increasing the network depth to 152 layers on ImageNet. Furthermore, according to the author's recent talk, they are able achieve 1000 layers on CIFAR-10 dataset. On the other hand, the author of WRN claims that a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks. Both methods have straightforward architecture, we believe it a good framework to explore the details in deep neural network. 

Combining the above two papers, we will study the effects of the filter size, batch normalization order, and dropout, with SGD solver for all the experiment settings. We are going to compare ``plain'' neural network, deep residual network and wide residual network. The expected results will be the consistent with the papers that deep residual network is better than ``plain'' network, while wide residual network achieves similar accuracy with much deeper residual network. The results will be displayed as training and testing error plot along with time. 

Due to the large size of ImageNet dataset, we will mainly work on CIFAR-10 dataset \cite{krizhevsky2009learning}, which contains 60000 labeled for 10 classes images $32\times32$ in size, training set has 50000 and test set 10000. The experiments will run on a NVIDIA TitanX GPU and the maximal network depth would roughly be 20. 

\bibliographystyle{plain}
\bibliography{proposal} 

\end{document}
